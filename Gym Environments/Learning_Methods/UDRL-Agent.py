"""Upside-Down.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bkLsIAljbOzlE6PJLqX9hck4bY0y5lCC
"""

import numpy as np
import torch
import torch.optim as optim
from gym_unity.envs import UnityToGymWrapper
from mlagents_envs.environment import UnityEnvironment
from mlagents_envs.side_channel import OutgoingMessage
from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel
import torch.nn.functional as F
import matplotlib.pyplot as plt
from Run_Gym import MySideChannel
from BehaviorFunctions import BF, ReplayBuffer

engineConfigChannel = EngineConfigurationChannel()
engineConfigChannel.set_configuration_parameters(capture_frame_rate=5)
customSideChannel = MySideChannel()
env = UnityEnvironment("./Mac Build/solidrally", side_channels=[engineConfigChannel, customSideChannel], worker_id=0, no_graphics=False)
env = UnityToGymWrapper(env, uint8_visual=False, allow_multiple_obs=True)

message = OutgoingMessage()
message.write_string("[Generate Arousal]:{}".format("false"))
customSideChannel.queue_message_to_send(message)

message = OutgoingMessage()
message.write_string("[Save Cells]:{}".format("false"))
customSideChannel.queue_message_to_send(message)

action_space = 9
state_space = env.observation_space[0].shape[0]
max_reward = 31*3

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

horizon_scale = 0.02
return_scale = 0.02
replay_size = 700
n_warm_up_episodes = 10
n_updates_per_iter = 100
n_episodes_per_iter = 15
last_few = 50
batch_size = 256
episode_length = 200

buffer = ReplayBuffer(replay_size)
bf = BF(state_space, action_space, 64, 1).to(device)
optimizer = optim.Adam(params=bf.parameters(), lr=1e-3)

samples = []
init_desired_reward = 1
init_time_horizon = 200

for i in range(n_warm_up_episodes):

    desired_return = torch.FloatTensor([init_desired_reward])
    target_return = int(desired_return) + 1
    desired_time_horizon = torch.FloatTensor([init_time_horizon])
    state = env.reset()[0]
    states = []
    actions = []
    rewards = []

    for j in range(episode_length):
        action = bf.action(torch.from_numpy(state).float().to(device), desired_return.to(device),
                           desired_time_horizon.to(device), return_scale, horizon_scale)

        reward = 0
        for _ in range(10):
            next_state, newReward, done, _ = env.step(action.cpu().numpy())
            reward += newReward

        states.append(torch.from_numpy(state).float())
        actions.append(action)
        rewards.append(reward)

        state = next_state[0]
        desired_return -= reward
        desired_time_horizon -= 1
        desired_time_horizon = torch.FloatTensor([np.maximum(desired_time_horizon, 1).item()])

        if target_return == np.sum(rewards):
            break

    buffer.add_sample(states, actions, rewards)


def sampling_exploration(top_X_eps=last_few):
    """
    This function calculates the new desired reward and new desired horizon based on the replay buffer.
    New desired horizon is calculted by the mean length of the best last X episodes.
    New desired reward is sampled from a uniform distribution given the mean and the std calculated from the last best X performances.
    where X is the hyperparameter last_few.
    """

    top_X = buffer.get_nbest(top_X_eps)
    # The exploratory desired horizon dh0 is set to the mean of the lengths of the selected episodes
    new_desired_horizon = np.mean([len(i["states"]) for i in top_X])
    # save all top_X cumulative returns in a list
    returns = [i["summed_rewards"] for i in top_X]
    # from these returns calc the mean and std
    mean_returns = np.mean(returns)
    std_returns = np.std(returns)
    # sample desired reward from a uniform distribution given the mean and the std
    new_desired_reward = np.random.uniform(mean_returns, mean_returns + std_returns)

    return torch.FloatTensor([np.round(new_desired_reward)]), torch.FloatTensor([new_desired_horizon])


def select_time_steps(saved_episode):
    """
    Given a saved episode from the replay buffer this function samples random time steps (t1 and t2) in that episode:
    T = max time horizon in that episode
    Returns t1, t2 and T
    """
    T = len(saved_episode["states"])  # episode max horizon
    t1 = np.random.randint(0, T - 1)
    t2 = np.random.randint(t1 + 1, T)
    return t1, t2, T


def create_training_input(episode, t1, t2):
    """
    Based on the selected episode and the given time steps this function returns 4 values:
    1. state at t1
    2. the desired reward: sum over all rewards from t1 to t2
    3. the time horizont: t2 -t1

    4. the target action taken at t1

    buffer episodes are build like [cumulative episode reward, states, actions, rewards]
    """
    state = episode["states"][t1]
    desired_reward = sum(episode["rewards"][t1:t2])
    time_horizont = t2 - t1
    action = episode["actions"][t1]
    return state, desired_reward, np.round(time_horizont), action


def create_training_examples(batch_size):
    """
    Creates a data set of training examples that can be used to create a data loader for training.
    ============================================================
    1. for the given batch_size episode idx are randomly selected
    2. based on these episodes t1 and t2 are samples for each selected episode
    3. for the selected episode and sampled t1 and t2 trainings values are gathered
    ______________________________________________________________
    Output are two numpy arrays in the length of batch size:
    Input Array for the Behavior function - consisting of (state, desired_reward, time_horizon)
    Output Array with the taken actions
    """
    input_array = []
    output_array = []
    # select randomly episodes from the buffer
    episodes = buffer.get_random_samples(batch_size)
    for ep in episodes:
        # select time stamps
        t1, t2, T = select_time_steps(ep)
        # For episodic tasks they set t2 to T:
        t2 = T
        state, desired_reward, time_horizont, action = create_training_input(ep, t1, t2)
        input_array.append(torch.cat([state, torch.FloatTensor([desired_reward]), torch.FloatTensor([time_horizont])]))
        output_array.append(action)
    return input_array, output_array


def train_behavior_function(batch_size):
    """
    Trains the BF with on a cross entropy loss were the inputs are the action probabilities based on the state and command.
    The targets are the actions appropriate to the states from the replay buffer.
    """
    X, y = create_training_examples(batch_size)
    X = torch.stack(X)
    state = X[:, 0:state_space]
    d = X[:, state_space:state_space + 1]
    h = X[:, state_space + 1:state_space + 2]
    command = torch.cat((d * return_scale, h * horizon_scale), dim=-1)
    y = torch.stack(y).long()
    y_ = bf(state.to(device), command.to(device)).float()
    optimizer.zero_grad()
    pred_loss = F.cross_entropy(y_, y)
    pred_loss.backward()
    optimizer.step()
    return pred_loss.detach().cpu().numpy()


def evaluate(desired_return=torch.FloatTensor([init_desired_reward]),
             desired_time_horizon=torch.FloatTensor([init_time_horizon])):
    """
    Runs one episode of the environment to evaluate the bf.
    """
    state = env.reset()
    rewards = 0
    for _ in range(episode_length):
        state = torch.FloatTensor(state[0])
        action = bf.action(state.to(device), desired_return.to(device), desired_time_horizon.to(device),  return_scale, horizon_scale)
        newReward = 0
        for _ in range(10):
            state, reward, done, _ = env.step(action.cpu().numpy())
            newReward += reward
        rewards += newReward
        desired_return = min(desired_return - newReward, torch.FloatTensor([max_reward]))
        desired_time_horizon = max(desired_time_horizon - 1, torch.FloatTensor([1]))

    return rewards


# Algorithm 2 - Generates an Episode unsing the Behavior Function:
def generate_episode(desired_return=torch.FloatTensor([init_desired_reward]),
                     desired_time_horizon=torch.FloatTensor([init_time_horizon])):
    """
    Generates more samples for the replay buffer.
    """
    state = env.reset()
    states = []
    actions = []
    rewards = []
    target_return = int(desired_return) + 1

    for _ in range(episode_length):
        state = torch.FloatTensor(state[0])
        action = bf.action(state.to(device), desired_return.to(device), desired_time_horizon.to(device), return_scale,
                           horizon_scale)

        reward = 0

        for j in range(10):
            next_state, thisReward, done, _ = env.step(action.cpu().numpy())
            reward += thisReward

        states.append(state)
        actions.append(action)
        rewards.append(reward)

        state = next_state
        desired_return -= reward
        desired_time_horizon -= 1
        desired_time_horizon = torch.FloatTensor([np.maximum(desired_time_horizon, 1).item()])

        if target_return == np.sum(rewards):
            break

    return [states, actions, rewards]


# Algorithm 1 - Upside - Down Reinforcement Learning
def run_upside_down(max_episodes):
    """

    """
    all_rewards = []
    losses = []
    average_100_reward = []
    desired_rewards_history = []
    horizon_history = []
    for ep in range(1, max_episodes + 1):

        # improve|optimize bf based on replay buffer
        loss_buffer = []
        for i in range(n_updates_per_iter):
            bf_loss = train_behavior_function(batch_size)
            loss_buffer.append(bf_loss)
        bf_loss = np.mean(loss_buffer)
        losses.append(bf_loss)

        # run x new episode and add to buffer
        for i in range(n_episodes_per_iter):
            # Sample exploratory commands based on buffer
            new_desired_reward, new_desired_horizon = sampling_exploration()
            generated_episode = generate_episode(new_desired_reward, new_desired_horizon)
            buffer.add_sample(generated_episode[0], generated_episode[1], generated_episode[2])

        new_desired_reward, new_desired_horizon = sampling_exploration()
        print(new_desired_reward, new_desired_horizon)

        # monitoring desired reward and desired horizon
        desired_rewards_history.append(new_desired_reward.item())
        horizon_history.append(new_desired_horizon.item())

        ep_rewards = evaluate(new_desired_reward, new_desired_horizon)
        all_rewards.append(ep_rewards)
        average_100_reward.append(np.mean(all_rewards[-100:]))

        print("\rEpisode: {} | Rewards: {:.2f} | Mean_100_Rewards: {:.2f} | Max_100_Rewards: {:.2f} | Loss: {:.2f}".format(ep, ep_rewards, np.mean(all_rewards[-100:]), np.max(all_rewards[-100:]), bf_loss), end="", flush=True)
        if ep % 100 == 0:
            print(
                "\rEpisode: {} | Rewards: {:.2f} | Mean_100_Rewards: {:.2f} | Max_100_Rewards: {:.2f} | Loss: {:.2f}".format(
                    ep, ep_rewards, np.mean(all_rewards[-100:]), np.max(all_rewards[-100:]), bf_loss))

    return all_rewards, average_100_reward, desired_rewards_history, horizon_history, losses


# Commented out IPython magic to ensure Python compatibility.
# %%time
rewards, average, d, h, loss = run_upside_down(max_episodes=500)
torch.save(bf.state_dict(), "behaviorfunction.pth")

plt.figure(figsize=(15, 8))
plt.subplot(2, 2, 1)
plt.title("Rewards")
plt.plot(rewards, label="rewards")
plt.plot(average, label="average100")
plt.legend()

plt.subplot(2, 2, 2)
plt.title("Loss")
plt.plot(loss)
plt.subplot(2, 2, 3)

plt.title("desired Rewards")
plt.plot(d)
plt.subplot(2, 2, 4)

plt.title("desired Horizon")
plt.plot(h)
plt.show()

# SAVE MODEL
name = "model.pth"
torch.save(bf.state_dict(), name)

DESIRED_REWARD = torch.FloatTensor([16]).to(device)
DESIRED_HORIZON = torch.FloatTensor([2000]).to(device)
desired = DESIRED_REWARD.item()

env.reset()
rewards = 0
for _ in range(episode_length):
    command = torch.cat((DESIRED_REWARD * return_scale, DESIRED_HORIZON * horizon_scale), dim=-1)

    probs_logits = bf(torch.from_numpy(state[0]).float().to(device), command)
    probs = torch.softmax(probs_logits, dim=-1).detach().cpu()
    action = torch.argmax(probs).item()
    state, reward, done, info = env.step(action)
    rewards += reward
    DESIRED_REWARD -= reward
    DESIRED_HORIZON -= 1
    if DESIRED_REWARD == 0:
        break

print("Desired rewards: {} | after finishing one episode the agent received {} rewards".format(desired, rewards))
env.close()
